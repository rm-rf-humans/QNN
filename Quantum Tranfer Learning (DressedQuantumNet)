{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28","include_colab_link":true},"accelerator":"TPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10315236,"sourceType":"datasetVersion","datasetId":6386018}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pennylane torchcam torchvision","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azjMfRfd4fQh","outputId":"537fc6b8-e9c8-41e7-d2f2-64cc4c670236","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:33.017017Z","iopub.execute_input":"2024-12-28T17:01:33.017322Z","iopub.status.idle":"2024-12-28T17:01:36.159571Z","shell.execute_reply.started":"2024-12-28T17:01:33.017301Z","shell.execute_reply":"2024-12-28T17:01:36.158428Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pennylane in /usr/local/lib/python3.10/dist-packages (0.39.0)\nRequirement already satisfied: torchcam in /usr/local/lib/python3.10/dist-packages (0.4.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\nRequirement already satisfied: numpy<2.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\nRequirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.15.1)\nRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\nRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\nRequirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\nRequirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.7.0)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\nRequirement already satisfied: pennylane-lightning>=0.39 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.39.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.1)\nRequirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (2.4.1+cu121)\nRequirement already satisfied: Pillow!=9.2.0,>=8.4.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (10.4.0)\nRequirement already satisfied: matplotlib<4.0.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (3.7.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (1.13.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (2024.6.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (2.8.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.8.30)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.0->torchcam) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->torchcam) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.0.0->torchcam) (1.3.0)\n","output_type":"stream"}],"execution_count":210},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms, datasets\nimport torchvision\nfrom pennylane import numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.transforms import ToPILImage\nfrom torchcam.methods import GradCAM, GradCAMpp\nimport pennylane as qml\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport shutil\nimport torch\nimport os\nimport time\nimport copy\nfrom IPython.display import display\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import recall_score","metadata":{"id":"hK0fPmvqCWIh","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:36.161330Z","iopub.execute_input":"2024-12-28T17:01:36.161644Z","iopub.status.idle":"2024-12-28T17:01:36.168381Z","shell.execute_reply.started":"2024-12-28T17:01:36.161605Z","shell.execute_reply":"2024-12-28T17:01:36.167592Z"}},"outputs":[],"execution_count":211},{"cell_type":"code","source":"#os.environ[\"OMP_NUM_THREADS\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:36.170170Z","iopub.execute_input":"2024-12-28T17:01:36.170431Z","iopub.status.idle":"2024-12-28T17:01:36.186472Z","shell.execute_reply.started":"2024-12-28T17:01:36.170411Z","shell.execute_reply":"2024-12-28T17:01:36.185679Z"}},"outputs":[],"execution_count":212},{"cell_type":"code","source":"class BreastCancerDataset(Dataset):\n    def __init__(self, image_paths, transform=None):\n        self.image_paths = image_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"L\")\n        label = 0 if 'normal' in img_path else 1\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"id":"pzf75IRJah26","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:36.187755Z","iopub.execute_input":"2024-12-28T17:01:36.188042Z","iopub.status.idle":"2024-12-28T17:01:36.200699Z","shell.execute_reply.started":"2024-12-28T17:01:36.188018Z","shell.execute_reply":"2024-12-28T17:01:36.199998Z"}},"outputs":[],"execution_count":213},{"cell_type":"code","source":"normal_dir = \"/kaggle/input/thermal-breast-cancer/normal_segmented_parts\"\nabnormal_dir = \"/kaggle/input/thermal-breast-cancer/abnormal_segmented_parts\"\noutput_dir = \"output/\"\ntransform = transforms.Compose([transforms.ToTensor()])","metadata":{"id":"guXwYb2naoMZ","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:36.201535Z","iopub.execute_input":"2024-12-28T17:01:36.201743Z","iopub.status.idle":"2024-12-28T17:01:36.213227Z","shell.execute_reply.started":"2024-12-28T17:01:36.201716Z","shell.execute_reply":"2024-12-28T17:01:36.212601Z"}},"outputs":[],"execution_count":214},{"cell_type":"code","source":"def split_separate_folders(normal_dir, abnormal_dir, output_dir, train_ratio=0.7,\n                         val_ratio=0.15, test_ratio=0.15, random_state=42):\n\n    splits = ['train', 'val', 'test']\n    classes = ['normal', 'abnormal']\n\n    for split in splits:\n        for cls in classes:\n            os.makedirs(os.path.join(output_dir, split, cls), exist_ok=True)\n\n    results = {\n        'Class': [],\n        'Training': [],\n        'Validation': [],\n        'Testing': [],\n        'Total': []\n    }\n\n    for cls, src_dir in zip(classes, [normal_dir, abnormal_dir]):\n\n        images = [f for f in os.listdir(src_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n        total_images = len(images)\n\n\n        train_images, temp_images = train_test_split(\n            images,\n            train_size=train_ratio,\n            random_state=random_state\n        )\n\n        val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n        val_images, test_images = train_test_split(\n            temp_images,\n            train_size=val_ratio_adjusted,\n            random_state=random_state\n        )\n\n        for img, split_type in zip([train_images, val_images, test_images], splits):\n            for image_name in img:\n                shutil.copy2(\n                    os.path.join(src_dir, image_name),\n                    os.path.join(output_dir, split_type, cls, image_name)\n                )\n\n        results['Class'].append(cls)\n        results['Training'].append(len(train_images))\n        results['Validation'].append(len(val_images))\n        results['Testing'].append(len(test_images))\n        results['Total'].append(total_images)\n\n    summary_df = pd.DataFrame(results)\n    display(summary_df)\n\n    print(\"\\nSplit Percentages:\")\n    for split in ['Training', 'Validation', 'Testing']:\n        total = summary_df[split].sum()\n        overall_total = summary_df['Total'].sum()\n        print(f\"{split}: {total} images ({total/overall_total*100:.1f}%)\")\n\n    return summary_df","metadata":{"id":"QNamCt6FdCod","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:36.214035Z","iopub.execute_input":"2024-12-28T17:01:36.214224Z","iopub.status.idle":"2024-12-28T17:01:36.226705Z","shell.execute_reply.started":"2024-12-28T17:01:36.214207Z","shell.execute_reply":"2024-12-28T17:01:36.225873Z"}},"outputs":[],"execution_count":215},{"cell_type":"code","source":"summary = split_separate_folders(\n    normal_dir=normal_dir,\n    abnormal_dir=abnormal_dir,\n    output_dir=output_dir\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"JFaQX-Gc29kq","outputId":"140392e6-d0e8-4437-af72-acb2bcb996d9","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:36.227535Z","iopub.execute_input":"2024-12-28T17:01:36.227809Z","iopub.status.idle":"2024-12-28T17:01:38.117858Z","shell.execute_reply.started":"2024-12-28T17:01:36.227777Z","shell.execute_reply":"2024-12-28T17:01:38.116921Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"      Class  Training  Validation  Testing  Total\n0    normal       350          75       75    500\n1  abnormal       350          75       75    500","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>Training</th>\n      <th>Validation</th>\n      <th>Testing</th>\n      <th>Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>normal</td>\n      <td>350</td>\n      <td>75</td>\n      <td>75</td>\n      <td>500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abnormal</td>\n      <td>350</td>\n      <td>75</td>\n      <td>75</td>\n      <td>500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nSplit Percentages:\nTraining: 700 images (70.0%)\nValidation: 150 images (15.0%)\nTesting: 150 images (15.0%)\n","output_type":"stream"}],"execution_count":216},{"cell_type":"code","source":"for split in ['train', 'val', 'test']:\n    for cls in ['normal', 'abnormal']:\n        path = os.path.join(output_dir, split, cls)\n        num_images = len([f for f in os.listdir(path) if f.endswith(('.png', '.jpg', '.jpeg'))])","metadata":{"id":"2pAnLeQ32_iN","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.119957Z","iopub.execute_input":"2024-12-28T17:01:38.120167Z","iopub.status.idle":"2024-12-28T17:01:38.125463Z","shell.execute_reply.started":"2024-12-28T17:01:38.120148Z","shell.execute_reply":"2024-12-28T17:01:38.124736Z"}},"outputs":[],"execution_count":217},{"cell_type":"code","source":"class MedicalImageDataset(Dataset):\n\n    def __init__(self, data_dir, transform=None):\n\n        self.data_dir = data_dir\n        self.transform = transform\n        self.classes = ['normal', 'abnormal']\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n\n        self.images = []\n        self.labels = []\n\n        for cls in self.classes:\n            class_dir = os.path.join(data_dir, cls)\n            class_idx = self.class_to_idx[cls]\n\n            for img_name in os.listdir(class_dir):\n                if img_name.endswith(('.png', '.jpg', '.jpeg')):\n                    self.images.append(os.path.join(class_dir, img_name))\n                    self.labels.append(class_idx)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label = self.labels[idx]\n\n        image = Image.open(img_path).convert('L')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"id":"uHBqGzYJ3VoX","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.127336Z","iopub.execute_input":"2024-12-28T17:01:38.127646Z","iopub.status.idle":"2024-12-28T17:01:38.140091Z","shell.execute_reply.started":"2024-12-28T17:01:38.127617Z","shell.execute_reply":"2024-12-28T17:01:38.139333Z"}},"outputs":[],"execution_count":218},{"cell_type":"code","source":"torch.manual_seed(42)\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\nval_test_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])","metadata":{"id":"BSQy1HXU3gBT","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.141002Z","iopub.execute_input":"2024-12-28T17:01:38.141278Z","iopub.status.idle":"2024-12-28T17:01:38.155002Z","shell.execute_reply.started":"2024-12-28T17:01:38.141249Z","shell.execute_reply":"2024-12-28T17:01:38.154359Z"}},"outputs":[],"execution_count":219},{"cell_type":"code","source":"n_qubits = 4                \nnum_epochs = 40             \nq_depth = 1      \nbatch_size=60               \nq_delta = 0.01              ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.155893Z","iopub.execute_input":"2024-12-28T17:01:38.156211Z","iopub.status.idle":"2024-12-28T17:01:38.168110Z","shell.execute_reply.started":"2024-12-28T17:01:38.156182Z","shell.execute_reply":"2024-12-28T17:01:38.167339Z"}},"outputs":[],"execution_count":220},{"cell_type":"code","source":"def create_data_loaders(data_dir, batch_size=60, num_workers=2):\n\n    train_dataset = MedicalImageDataset(\n        os.path.join(data_dir, 'train'),\n        transform=train_transform\n    )\n\n    val_dataset = MedicalImageDataset(\n        os.path.join(data_dir, 'val'),\n        transform=val_test_transform\n    )\n\n    test_dataset = MedicalImageDataset(\n        os.path.join(data_dir, 'test'),\n        transform=val_test_transform\n    )\n\n    print(f\"Training set size: {len(train_dataset)}\")\n    print(f\"Validation set size: {len(val_dataset)}\")\n    print(f\"Test set size: {len(test_dataset)}\")\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n\n    return train_loader, val_loader, test_loader","metadata":{"id":"ByHckTvi3kZT","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.169495Z","iopub.execute_input":"2024-12-28T17:01:38.169794Z","iopub.status.idle":"2024-12-28T17:01:38.183735Z","shell.execute_reply.started":"2024-12-28T17:01:38.169755Z","shell.execute_reply":"2024-12-28T17:01:38.183098Z"}},"outputs":[],"execution_count":221},{"cell_type":"code","source":"data_dir = \"output\"\n\ntrain_loader, val_loader, test_loader = create_data_loaders(\n    data_dir=data_dir,\n    batch_size=batch_size\n)\n\n\nfor images, labels in train_loader:\n    print(f\"Batch shape: {images.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n    print(f\"Labels: {labels}\")\n    break","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3gnXqPCPfoFU","outputId":"b898e2ea-0ca7-47b9-a6bc-8f629e0c15cf","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.184384Z","iopub.execute_input":"2024-12-28T17:01:38.184636Z","iopub.status.idle":"2024-12-28T17:01:38.455178Z","shell.execute_reply.started":"2024-12-28T17:01:38.184616Z","shell.execute_reply":"2024-12-28T17:01:38.454332Z"}},"outputs":[{"name":"stdout","text":"Training set size: 700\nValidation set size: 150\nTest set size: 150\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Batch shape: torch.Size([60, 1, 224, 224])\nLabels shape: torch.Size([60])\nLabels: tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1])\n","output_type":"stream"}],"execution_count":222},{"cell_type":"code","source":"def H_layer(nqubits):\n    \"\"\"Layer of single-qubit Hadamard gates.\n    \"\"\"\n    for idx in range(nqubits):\n        qml.Hadamard(wires=idx)\n\n\ndef RY_layer(w):\n    \"\"\"Layer of parametrized qubit rotations around the y axis.\n    \"\"\"\n    for idx, element in enumerate(w):\n        qml.RY(element, wires=idx)\n\n\ndef entangling_layer(nqubits):\n    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n    \"\"\"\n    # In other words it should apply something like :\n    # CNOT  CNOT  CNOT  CNOT...  CNOT\n    #   CNOT  CNOT  CNOT...  CNOT\n    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n        qml.CNOT(wires=[i, i + 1])\n    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n        qml.CNOT(wires=[i, i + 1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.456375Z","iopub.execute_input":"2024-12-28T17:01:38.456720Z","iopub.status.idle":"2024-12-28T17:01:38.462863Z","shell.execute_reply.started":"2024-12-28T17:01:38.456687Z","shell.execute_reply":"2024-12-28T17:01:38.462052Z"}},"outputs":[],"execution_count":223},{"cell_type":"code","source":"dev = qml.device(\"default.qubit\", wires=4)\n\n\n@qml.qnode(dev)\ndef quantum_net(q_input_features, q_weights_flat):\n    \"\"\"\n    The variational quantum circuit.\n    \"\"\"\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    RY_layer(q_input_features)\n\n    # Sequence of trainable variational layers\n    for k in range(q_depth):\n        entangling_layer(n_qubits)\n        RY_layer(q_weights[k])\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.463911Z","iopub.execute_input":"2024-12-28T17:01:38.464252Z","iopub.status.idle":"2024-12-28T17:01:38.482537Z","shell.execute_reply.started":"2024-12-28T17:01:38.464223Z","shell.execute_reply":"2024-12-28T17:01:38.481871Z"}},"outputs":[],"execution_count":224},{"cell_type":"code","source":"class DressedQuantumNet(nn.Module):\n    \"\"\"\n    Torch module implementing the *dressed* quantum net.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Definition of the *dressed* layout.\n        \"\"\"\n\n        super().__init__()\n        self.pre_net = nn.Linear(512, n_qubits)\n        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n        self.post_net = nn.Linear(n_qubits, 2)\n\n    def forward(self, input_features):\n        \"\"\"\n        Defining how tensors are supposed to move through the *dressed* quantum\n        net.\n        \"\"\"\n\n        # obtain the input features for the quantum circuit\n        # by reducing the feature dimension from 512 to 4\n        pre_out = self.pre_net(input_features)\n        q_in = torch.tanh(pre_out) * np.pi / 2.0\n\n        # Apply the quantum circuit to each element of the batch and append to q_out\n        q_out = torch.Tensor(0, n_qubits)\n        q_out = q_out.to(device)\n        for elem in q_in:\n            q_out_elem = torch.hstack(quantum_net(elem, self.q_params)).float().unsqueeze(0)\n            q_out = torch.cat((q_out, q_out_elem))\n\n        # return the two-dimensional prediction from the postprocessing layer\n        return self.post_net(q_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.483322Z","iopub.execute_input":"2024-12-28T17:01:38.483578Z","iopub.status.idle":"2024-12-28T17:01:38.496311Z","shell.execute_reply.started":"2024-12-28T17:01:38.483558Z","shell.execute_reply":"2024-12-28T17:01:38.495582Z"}},"outputs":[],"execution_count":225},{"cell_type":"code","source":"weights = torchvision.models.ResNet18_Weights.IMAGENET1K_V1\nmodel_hybrid = torchvision.models.resnet18(weights=weights)\n\nfor param in model_hybrid.parameters():\n    param.requires_grad = False\n\n\n# Notice that model_hybrid.fc is the last layer of ResNet18\nmodel_hybrid.fc = DressedQuantumNet()\n\n# Use CUDA or CPU according to the \"device\" object.\nmodel_hybrid = model_hybrid.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.497181Z","iopub.execute_input":"2024-12-28T17:01:38.497422Z","iopub.status.idle":"2024-12-28T17:01:38.755827Z","shell.execute_reply.started":"2024-12-28T17:01:38.497397Z","shell.execute_reply":"2024-12-28T17:01:38.755033Z"}},"outputs":[],"execution_count":226},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    best_loss = 10000.0  # Large arbitrary number\n    best_acc_train = 0.0\n    best_loss_train = 10000.0  # Large arbitrary number\n    print(\"Training started:\")\n\n    for epoch in range(num_epochs):\n\n        # Each epoch has a training and validation phase\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                # Set model to training mode\n                model.train()\n            else:\n                # Set model to evaluate mode\n                model.eval()\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            n_batches = dataset_sizes[phase] // batch_size\n            it = 0\n            for inputs, labels in dataloaders[phase]:\n                since_batch = time.time()\n                batch_size_ = len(inputs)\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                optimizer.zero_grad()\n\n                # Track/compute gradient and make an optimization step only when training\n                with torch.set_grad_enabled(phase == \"train\"):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n\n                # Print iteration results\n                running_loss += loss.item() * batch_size_\n                batch_corrects = torch.sum(preds == labels.data).item()\n                running_corrects += batch_corrects\n                print(\n                    \"Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}\".format(\n                        phase,\n                        epoch + 1,\n                        num_epochs,\n                        it + 1,\n                        n_batches + 1,\n                        time.time() - since_batch,\n                    ),\n                    end=\"\\r\",\n                    flush=True,\n                )\n                it += 1\n\n            # Print epoch results\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n            print(\n                \"Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}        \".format(\n                    \"train\" if phase == \"train\" else \"val  \",\n                    epoch + 1,\n                    num_epochs,\n                    epoch_loss,\n                    epoch_acc,\n                )\n            )\n\n            # Check if this is the best model wrt previous epochs\n            if phase == \"val\" and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == \"val\" and epoch_loss < best_loss:\n                best_loss = epoch_loss\n            if phase == \"train\" and epoch_acc > best_acc_train:\n                best_acc_train = epoch_acc\n            if phase == \"train\" and epoch_loss < best_loss_train:\n                best_loss_train = epoch_loss\n\n            # Update learning rate\n            if phase == \"train\":\n                scheduler.step()\n\n    # Print final results\n    model.load_state_dict(best_model_wts)\n    time_elapsed = time.time() - since\n    print(\n        \"Training completed in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n    )\n    print(\"Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.756680Z","iopub.execute_input":"2024-12-28T17:01:38.757013Z","iopub.status.idle":"2024-12-28T17:01:38.766792Z","shell.execute_reply.started":"2024-12-28T17:01:38.756979Z","shell.execute_reply":"2024-12-28T17:01:38.766045Z"}},"outputs":[],"execution_count":227},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer_hybrid = optim.Adam(model_hybrid.fc.parameters(), lr=0.001, weight_decay=1e-5)\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_hybrid, step_size=10, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.767506Z","iopub.execute_input":"2024-12-28T17:01:38.767726Z","iopub.status.idle":"2024-12-28T17:01:38.785482Z","shell.execute_reply.started":"2024-12-28T17:01:38.767708Z","shell.execute_reply":"2024-12-28T17:01:38.784654Z"}},"outputs":[],"execution_count":228},{"cell_type":"code","source":"model_hybrid = train_model(\n    model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:01:38.786376Z","iopub.execute_input":"2024-12-28T17:01:38.786665Z","iopub.status.idle":"2024-12-28T17:10:59.760222Z","shell.execute_reply.started":"2024-12-28T17:01:38.786635Z","shell.execute_reply":"2024-12-28T17:10:59.759467Z"}},"outputs":[{"name":"stdout","text":"Training started:\nPhase: train Epoch: 1/40 Loss: 0.5574 Acc: 0.6957        \nPhase: val   Epoch: 1/40 Loss: 0.3750 Acc: 0.8733        \nPhase: train Epoch: 2/40 Loss: 0.4200 Acc: 0.8314        \nPhase: val   Epoch: 2/40 Loss: 0.2847 Acc: 0.9200        \nPhase: train Epoch: 3/40 Loss: 0.3793 Acc: 0.8486        \nPhase: val   Epoch: 3/40 Loss: 0.2489 Acc: 0.9267        \nPhase: train Epoch: 4/40 Loss: 0.3663 Acc: 0.8386        \nPhase: val   Epoch: 4/40 Loss: 0.1963 Acc: 0.9800        \nPhase: train Epoch: 5/40 Loss: 0.3370 Acc: 0.8643        \nPhase: val   Epoch: 5/40 Loss: 0.1846 Acc: 0.9533        \nPhase: train Epoch: 6/40 Loss: 0.3125 Acc: 0.8714        \nPhase: val   Epoch: 6/40 Loss: 0.1312 Acc: 0.9800        \nPhase: train Epoch: 7/40 Loss: 0.2732 Acc: 0.8914        \nPhase: val   Epoch: 7/40 Loss: 0.1571 Acc: 0.9533        \nPhase: train Epoch: 8/40 Loss: 0.3260 Acc: 0.8629        \nPhase: val   Epoch: 8/40 Loss: 0.1149 Acc: 0.9867        \nPhase: train Epoch: 9/40 Loss: 0.2625 Acc: 0.8957        \nPhase: val   Epoch: 9/40 Loss: 0.1028 Acc: 0.9933        \nPhase: train Epoch: 10/40 Loss: 0.3201 Acc: 0.8657        \nPhase: val   Epoch: 10/40 Loss: 0.1452 Acc: 0.9733        \nPhase: train Epoch: 11/40 Loss: 0.2681 Acc: 0.8929        \nPhase: val   Epoch: 11/40 Loss: 0.1036 Acc: 0.9867        \nPhase: train Epoch: 12/40 Loss: 0.2446 Acc: 0.9043        \nPhase: val   Epoch: 12/40 Loss: 0.0961 Acc: 0.9867        \nPhase: train Epoch: 13/40 Loss: 0.2575 Acc: 0.9057        \nPhase: val   Epoch: 13/40 Loss: 0.0941 Acc: 0.9933        \nPhase: train Epoch: 14/40 Loss: 0.2599 Acc: 0.9086        \nPhase: val   Epoch: 14/40 Loss: 0.0946 Acc: 0.9933        \nPhase: train Epoch: 15/40 Loss: 0.2237 Acc: 0.9100        \nPhase: val   Epoch: 15/40 Loss: 0.0863 Acc: 0.9933        \nPhase: train Epoch: 16/40 Loss: 0.2360 Acc: 0.9071        \nPhase: val   Epoch: 16/40 Loss: 0.0891 Acc: 0.9933        \nPhase: train Epoch: 17/40 Loss: 0.2408 Acc: 0.9000        \nPhase: val   Epoch: 17/40 Loss: 0.0891 Acc: 0.9933        \nPhase: train Epoch: 18/40 Loss: 0.2251 Acc: 0.9057        \nPhase: val   Epoch: 18/40 Loss: 0.0956 Acc: 0.9867        \nPhase: train Epoch: 19/40 Loss: 0.2606 Acc: 0.9014        \nPhase: val   Epoch: 19/40 Loss: 0.0908 Acc: 0.9867        \nPhase: train Epoch: 20/40 Loss: 0.2281 Acc: 0.9129        \nPhase: val   Epoch: 20/40 Loss: 0.0796 Acc: 0.9933        \nPhase: train Epoch: 21/40 Loss: 0.2512 Acc: 0.9000        \nPhase: val   Epoch: 21/40 Loss: 0.0830 Acc: 0.9933        \nPhase: train Epoch: 22/40 Loss: 0.2231 Acc: 0.9129        \nPhase: val   Epoch: 22/40 Loss: 0.0868 Acc: 0.9933        \nPhase: train Epoch: 23/40 Loss: 0.2568 Acc: 0.9029        \nPhase: val   Epoch: 23/40 Loss: 0.0803 Acc: 0.9933        \nPhase: train Epoch: 24/40 Loss: 0.2012 Acc: 0.9171        \nPhase: val   Epoch: 24/40 Loss: 0.0825 Acc: 0.9933        \nPhase: train Epoch: 25/40 Loss: 0.2094 Acc: 0.9029        \nPhase: val   Epoch: 25/40 Loss: 0.0808 Acc: 0.9933        \nPhase: train Epoch: 26/40 Loss: 0.2117 Acc: 0.9214        \nPhase: val   Epoch: 26/40 Loss: 0.0875 Acc: 0.9867        \nPhase: train Epoch: 27/40 Loss: 0.2133 Acc: 0.9186        \nPhase: val   Epoch: 27/40 Loss: 0.0854 Acc: 0.9867        \nPhase: train Epoch: 28/40 Loss: 0.2275 Acc: 0.8914        \nPhase: val   Epoch: 28/40 Loss: 0.0795 Acc: 0.9933        \nPhase: train Epoch: 29/40 Loss: 0.2158 Acc: 0.9243        \nPhase: val   Epoch: 29/40 Loss: 0.0783 Acc: 0.9933        \nPhase: train Epoch: 30/40 Loss: 0.2131 Acc: 0.9143        \nPhase: val   Epoch: 30/40 Loss: 0.0797 Acc: 0.9867        \nPhase: train Epoch: 31/40 Loss: 0.2210 Acc: 0.9200        \nPhase: val   Epoch: 31/40 Loss: 0.0837 Acc: 0.9867        \nPhase: train Epoch: 32/40 Loss: 0.2251 Acc: 0.9043        \nPhase: val   Epoch: 32/40 Loss: 0.0857 Acc: 0.9933        \nPhase: train Epoch: 33/40 Loss: 0.2506 Acc: 0.8971        \nPhase: val   Epoch: 33/40 Loss: 0.0815 Acc: 0.9933        \nPhase: train Epoch: 34/40 Loss: 0.2440 Acc: 0.8957        \nPhase: val   Epoch: 34/40 Loss: 0.0797 Acc: 0.9933        \nPhase: train Epoch: 35/40 Loss: 0.2333 Acc: 0.9157        \nPhase: val   Epoch: 35/40 Loss: 0.0833 Acc: 0.9933        \nPhase: train Epoch: 36/40 Loss: 0.2138 Acc: 0.9229        \nPhase: val   Epoch: 36/40 Loss: 0.0811 Acc: 0.9933        \nPhase: train Epoch: 37/40 Loss: 0.2425 Acc: 0.8871        \nPhase: val   Epoch: 37/40 Loss: 0.0801 Acc: 0.9933        \nPhase: train Epoch: 38/40 Loss: 0.2241 Acc: 0.9186        \nPhase: val   Epoch: 38/40 Loss: 0.0829 Acc: 0.9933        \nPhase: train Epoch: 39/40 Loss: 0.2398 Acc: 0.9086        \nPhase: val   Epoch: 39/40 Loss: 0.0848 Acc: 0.9933        \nPhase: train Epoch: 40/40 Loss: 0.2371 Acc: 0.9029        \nPhase: val   Epoch: 40/40 Loss: 0.0814 Acc: 0.9933        \nTraining completed in 9m 21s\nBest test loss: 0.0783 | Best test accuracy: 0.9933\n","output_type":"stream"}],"execution_count":229},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport cv2\n\nclass ModelWrapper(nn.Module):\n    def __init__(self, model):\n        super(ModelWrapper, self).__init__()\n        self.model = model\n        self.gradients = None\n        self.activations = None\n        self.gradients_squared = None\n        self.activations_squared = None\n\n        self.model.feature_extractor.conv2.register_forward_hook(self.activations_hook)\n        self.model.feature_extractor.conv2.register_backward_hook(self.gradients_hook)\n    \n    def activations_hook(self, module, input, output):\n        self.activations = output\n        self.activations_squared = output ** 2\n    \n    def gradients_hook(self, module, grad_input, grad_output):\n        self.gradients = grad_output[0]\n        self.gradients_squared = grad_output[0] ** 2\n    \n    def forward(self, x):\n        return self.model(x)\n    \n    def get_gradcam(self, input_image, target_class=None):\n        output = self.forward(input_image)\n        \n        if target_class is None:\n            target_class = output.argmax(dim=1).item()\n        \n        self.model.zero_grad()\n        \n        one_hot = torch.zeros_like(output)\n        one_hot[0][target_class] = 1\n        output.backward(gradient=one_hot)\n        \n        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n        \n        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n        cam = F.relu(cam)\n        \n        cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n        cam = cam - cam.min()\n        cam = cam / (cam.max() + 1e-7)\n        \n        return cam.squeeze().detach().cpu().numpy(), target_class, output\n    \n    def get_gradcampp(self, input_image, target_class=None):\n        output = self.forward(input_image)\n        \n        if target_class is None:\n            target_class = output.argmax(dim=1).item()\n        \n        self.model.zero_grad()\n        \n        one_hot = torch.zeros_like(output)\n        one_hot[0][target_class] = 1\n        output.backward(gradient=one_hot)\n        \n        alpha_num = self.gradients_squared\n        alpha_denom = 2 * self.gradients_squared + torch.sum(\n            self.activations * self.gradients, dim=(2, 3), keepdim=True\n        )\n        alpha_denom = torch.where(alpha_denom != 0, alpha_denom, torch.ones_like(alpha_denom))\n        \n        alphas = alpha_num / alpha_denom\n        weights = torch.sum(alphas * F.relu(self.gradients), dim=(2, 3), keepdim=True)\n        \n        campp = torch.sum(weights * self.activations_squared, dim=1, keepdim=True)\n        campp = F.relu(campp)\n        \n        campp = F.interpolate(campp, size=(224, 224), mode='bilinear', align_corners=False)\n        campp = campp - campp.min()\n        campp = campp / (campp.max() + 1e-7)\n        \n        return campp.squeeze().detach().cpu().numpy(), target_class, output\n    \n    def get_guided_backprop(self, input_image, target_class=None):\n        def guided_hook(module, grad_in, grad_out):\n            \n            return (F.relu(grad_in[0]), None, None)\n        \n        # Register the guided hook\n        self.model.feature_extractor.conv1.register_backward_hook(guided_hook)\n        \n        input_image.requires_grad = True\n        output = self.forward(input_image)\n        \n        if target_class is None:\n            target_class = output.argmax(dim=1).item()\n        \n        self.model.zero_grad()\n        \n        one_hot = torch.zeros_like(output)\n        one_hot[0][target_class] = 1\n        output.backward(gradient=one_hot)\n        \n        guided_gradients = input_image.grad.data\n        guided_gradients = guided_gradients - guided_gradients.min()\n        guided_gradients = guided_gradients / (guided_gradients.max() + 1e-7)\n        \n        return guided_gradients.squeeze().detach().cpu().numpy(), target_class, output\n    \n    def get_guided_gradcampp(self, input_image, target_class=None):\n        campp, target_class, output = self.get_gradcampp(input_image, target_class)\n        \n        guided_gradients, _, _ = self.get_guided_backprop(input_image, target_class)\n        \n        guided_gradcampp = guided_gradients * campp\n        \n        return guided_gradcampp, target_class, output\n\ndef visualize_gradcam(image_tensor, guided_cam, guided_campp, cam, campp, target_class, output, class_names=None):\n    image = image_tensor.squeeze().detach().cpu().numpy()\n    \n    heatmap_cam = np.uint8(255 * cam)\n    heatmap_cam = cv2.applyColorMap(heatmap_cam, cv2.COLORMAP_JET)\n    \n    heatmap_campp = np.uint8(255 * campp)\n    heatmap_campp = cv2.applyColorMap(heatmap_campp, cv2.COLORMAP_JET)\n    \n    probabilities = torch.nn.functional.softmax(output, dim=1)\n    prob = probabilities[0][target_class].item()\n    \n    if class_names and target_class < len(class_names):\n        class_label = f\"Class: {class_names[target_class]}\"\n    else:\n        class_label = f\"Class: {target_class}\"\n    \n    fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n    \n    axes[0].imshow(image, cmap='gray')\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n    \n    axes[1].imshow(guided_cam, cmap='gray')\n    axes[1].set_title('Guided GradCAM')\n    axes[1].axis('off')\n    \n    axes[2].imshow(guided_campp, cmap='gray')\n    axes[2].set_title('Guided GradCAM++')\n    axes[2].axis('off')\n    \n    axes[3].imshow(cam, cmap='jet')\n    axes[3].set_title(f'GradCAM\\n{class_label}\\nProbability: {prob:.3f}')\n    axes[3].axis('off')\n    \n    axes[4].imshow(campp, cmap='jet')\n    axes[4].set_title(f'GradCAM++\\n{class_label}\\nProbability: {prob:.3f}')\n    axes[4].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef compute_and_show_gradcam(model_path, image_path, target_class=None, class_names=None): \n    model = HybridModel()\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    \n    wrapped_model = ModelWrapper(model)\n    \n    transform = transforms.Compose([\n        transforms.Grayscale(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n    ])\n    \n    image = Image.open(image_path)\n    image_tensor = transform(image).unsqueeze(0)\n    image_tensor.requires_grad = True\n    \n    cam, pred_class, output = wrapped_model.get_gradcam(image_tensor, target_class)\n    campp, pred_class_pp, output_pp = wrapped_model.get_gradcampp(image_tensor, target_class)\n    guided_cam, _, _ = wrapped_model.get_guided_backprop(image_tensor, target_class)\n    guided_campp, _, _ = wrapped_model.get_guided_gradcampp(image_tensor, target_class)\n    \n    visualize_gradcam(image_tensor, guided_cam, guided_campp, cam, campp, pred_class, output, class_names)\n    \n    return cam, campp, guided_cam, guided_campp, pred_class, output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 30\nbest_val_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    model_hybrid.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model_hybrid(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = correct / total\n\n    mode_hybrid.eval()\n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for val_inputs, val_labels in val_loader:\n            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n\n            val_outputs = model_hybrid(val_inputs)\n            val_loss = criterion(val_outputs, val_labels)\n            val_running_loss += val_loss.item()\n            _, val_predicted = torch.max(val_outputs.data, 1)\n            val_total += val_labels.size(0)\n            val_correct += (val_predicted == val_labels).sum().item()\n\n    val_loss_avg = val_running_loss / len(val_loader)\n    val_accuracy = val_correct / val_total\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n          f\"Val Loss: {val_loss_avg:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n\n    if val_loss_avg < best_val_loss:\n        best_val_loss = val_loss_avg\n        torch.save(model_hybrid.state_dict(), 'best_model.pth')\n\n    scheduler.step()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVVpksUYNODQ","outputId":"26774f90-2597-43b1-f43a-db31a6c4a9b5","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:10:59.761073Z","iopub.execute_input":"2024-12-28T17:10:59.761355Z","iopub.status.idle":"2024-12-28T17:11:00.053176Z","shell.execute_reply.started":"2024-12-28T17:10:59.761325Z","shell.execute_reply":"2024-12-28T17:11:00.051466Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-230-4ef14f85a244>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_hybrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 454\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    455\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[60, 1, 224, 224] to have 3 channels, but got 1 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [64, 3, 7, 7], expected input[60, 1, 224, 224] to have 3 channels, but got 1 channels instead","output_type":"error"}],"execution_count":230},{"cell_type":"code","source":"def load_random_image(image_folder_path):\n    image_files = [f for f in os.listdir(image_folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n    random_image_path = os.path.join(image_folder_path, random.choice(image_files))\n    image = Image.open(random_image_path).convert('L')\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5], std=[0.5])\n    ])\n    image = transform(image)\n    return image\n\ndef test_random_image(model, image_folder_path):\n    model.eval()\n\n    image = load_random_image(image_folder_path)\n    image = image.unsqueeze(0).to(device)\n    with torch.no_grad():\n              output = model(image)\n\n    _, predicted_class = torch.max(output, 1)\n    predicted_class = predicted_class.item()\n\n    print(f\"Predicted class for the random image: {'Cancer' if predicted_class == 1 else 'No Cancer'}\")\n\nimage_folder_path = 'output/test/normal'\ntest_random_image(model_hybrid, image_folder_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPcEkeLsVVdW","outputId":"8101a639-e132-4258-b588-0a708500d076","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:11:00.053795Z","iopub.status.idle":"2024-12-28T17:11:00.054101Z","shell.execute_reply":"2024-12-28T17:11:00.053974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\ntest_running_loss = 0.0\ntest_correct = 0\ntest_total = 0\n\nwith torch.no_grad():\n    for test_inputs, test_labels in test_loader:\n        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n\n        test_outputs = model(test_inputs)\n\n        test_loss = criterion(test_outputs, test_labels)\n\n        test_running_loss += test_loss.item()\n        _, test_predicted = torch.max(test_outputs.data, 1)\n        test_total += test_labels.size(0)\n        test_correct += (test_predicted == test_labels).sum().item()\n\ntest_loss_avg = test_running_loss / len(test_loader)\ntest_accuracy = test_correct / test_total\n\nprint(f\"Test Loss: {test_loss_avg:.4f}, Test Accuracy: {test_accuracy:.4f}\")","metadata":{"id":"Niu8f9g1_lHc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1437434c-223f-42a6-ea1b-c878f8e90342","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:11:00.054793Z","iopub.status.idle":"2024-12-28T17:11:00.055185Z","shell.execute_reply":"2024-12-28T17:11:00.055020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(model, test_loader, device)\nevaluate_model(model, val_loader, device)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"IIw1yER8jjpn","outputId":"d2062d0d-df3e-4049-9600-c60841afba06","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T17:11:00.056046Z","iopub.status.idle":"2024-12-28T17:11:00.056366Z","shell.execute_reply":"2024-12-28T17:11:00.056252Z"}},"outputs":[],"execution_count":null}]}